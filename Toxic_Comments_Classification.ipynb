{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Comments Classification.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPg9RV2a7cqg"
      },
      "source": [
        "Toxic Comments Classification using 1D Convolution with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDAW5Z6D_0nX"
      },
      "source": [
        "1) Import Packages and *Functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fOnAQTxCKUe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow .keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "import io \n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av5y-38pGDnb"
      },
      "source": [
        "# Check tf version\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXpzWbcAK-h"
      },
      "source": [
        "2) Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPB1OYD_8drI"
      },
      "source": [
        "train_df = pd.read_csv('/content/sample_data/train.csv') .fillna(' ')\n",
        "train_df.sample(5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X45AcLoSUmsf"
      },
      "source": [
        "# View comment_text column\n",
        "X = train_df['comment_text'].values\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJxsaNtrvJ4D"
      },
      "source": [
        "3) Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oppMaX7dVyYe"
      },
      "source": [
        "# View few toxic comments\n",
        "train_df.loc[train_df['toxic']==1].sample(5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui2XxFzmXPMC"
      },
      "source": [
        "# Create wordcloud for toxic word collection\n",
        "comments = train_df['comment_text'].loc[train_df['toxic']==1].values\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width = 640,\n",
        "    height = 640,\n",
        "    background_color='black',\n",
        "    stopwords = STOPWORDS).generate(str(comments))\n",
        "\n",
        "fig = plt.figure(\n",
        "    figsize=(5,5),\n",
        "    facecolor='w',\n",
        "    edgecolor='w'\n",
        ")\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBrfBDJ4Zurx"
      },
      "source": [
        "# View toxic values\n",
        "y = train_df['toxic'].values\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JStgTIJ5b4zH"
      },
      "source": [
        "# Plot histogram for toxic column\n",
        "train_df['toxic'].plot(kind='hist', title='Distribution of Toxic Comments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MXumt1cPiI"
      },
      "source": [
        "# Count the values of 'toxic' and 'non toxic'\n",
        "train_df['toxic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af1Z-2Yg4eN"
      },
      "source": [
        "4) Data Preparation__ Tokenization and Pad Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOwD1Fspgfhi"
      },
      "source": [
        "max_features = 20000\n",
        "max_text_length = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm-eALg2rW0Q"
      },
      "source": [
        "x_tokenizer = text.Tokenizer(max_features)\n",
        "x_tokenizer.fit_on_texts(list(X))\n",
        "\n",
        "x_tokenized = x_tokenizer.texts_to_sequences(X)\n",
        "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWPQWlDuwISF"
      },
      "source": [
        "5) Prepare Embedding Matrix with Pre-trained GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQUlcHCJsKYL"
      },
      "source": [
        "embedding_dim = 100\n",
        "embeddings_index = dict()\n",
        "f = open('/content/sample_data/glove.6B.100d.txt')\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coef = np.asarray(values[1:], dtype = 'float32')\n",
        "  embeddings_index[word] = coef \n",
        "f.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDpXAh85w112"
      },
      "source": [
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, index in x_tokenizer.word_index.items():\n",
        "  if index > max_features -1 :\n",
        "    break\n",
        "  else:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA9fIyAazHtn"
      },
      "source": [
        "6) Create the Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w113e6KMwkEM"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, \n",
        "                    embedding_dim,\n",
        "                    embeddings_initializer = tf.keras.initializers.Constant(\n",
        "                    embedding_matrix),\n",
        "                    trainable = False))\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4OMso_CRQyh"
      },
      "source": [
        "7) Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XLkeBRLzFQz"
      },
      "source": [
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qmuIlyBFe-F"
      },
      "source": [
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding = 'valid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ArcDCO4RvI6"
      },
      "source": [
        "model.add(MaxPool1D())\n",
        "model.add(Conv1D(filters,\n",
        "                 5,\n",
        "                 padding = 'valid',\n",
        "                 activation = 'relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_dims, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Le4vL8nU_v8"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH4iKanpWjrt"
      },
      "source": [
        "8) Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnORWs2eWMtM"
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y,\n",
        "                                                  test_size = 0.15, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-1iYlkOXTSv"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = 3,\n",
        "          validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCtQa9Eer1VP"
      },
      "source": [
        "9) Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6c6EzQoYB4A"
      },
      "source": [
        "test_df = pd.read_csv('/content/sample_data/test.csv')\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_20QzqgwfY9"
      },
      "source": [
        "x_test = test_df['comment_text'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQJVWg8U_o2_"
      },
      "source": [
        "x_text_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
        "x_testing = sequence.pad_sequences(x_text_tokenized, maxlen = max_text_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFkbWvO1KdJL"
      },
      "source": [
        "y_testing = model.predict(x_testing, verbose = 1, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLr38C_RKyoM"
      },
      "source": [
        "y_testing.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7IP15xK1HO"
      },
      "source": [
        "y_testing[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMXxCBwK5Gy"
      },
      "source": [
        "test_df['Toxic'] = ['not toxic' if x < .5 else 'toxic' for x in y_testing]\n",
        "test_df[['comment_text', 'Toxic']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}